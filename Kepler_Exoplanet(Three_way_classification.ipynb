{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hazieon/Exoplanet-Detection/blob/dev/Kepler_Exoplanet(Three_way_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e59b6e44"
      },
      "outputs": [],
      "source": [
        "#connecting to google drive to bring in the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Access the full directory\n",
        "# https://drive.google.com/drive/folders/1ZWVwwQ1cvAenlEz5nz-HpOjhW-EdmZkR?usp=sharing\n",
        "# https://github.com/hazieon/Exoplanet-Detection/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZyC5OLqsVXr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, ReLU, LeakyReLU\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image, display\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT7tLzD6snoO",
        "outputId": "1c54e960-2fa5-48fe-8d0c-7db0636b940c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set distribution:\n",
            " ExoplanetClass\n",
            "0    1688\n",
            "1    1009\n",
            "2     214\n",
            "Name: count, dtype: int64\n",
            "Testing set distribution:\n",
            " ExoplanetClass\n",
            "0    563\n",
            "1    336\n",
            "2     72\n",
            "Name: count, dtype: int64\n",
            "✅ New scaler created and saved successfully!\n",
            "Saved to: /content/drive/MyDrive/KeplerExoplanet/kepler_exoplanet_scaler.pkl\n",
            "Scaler fitted on 76 features.\n",
            "First 10 features: ['default_flag' 'sy_snum' 'sy_pnum' 'disc_year' 'pl_controv_flag'\n",
            " 'pl_orbper' 'pl_orbpererr1' 'pl_orbpererr2' 'pl_orbperlim' 'pl_orbsmax']\n",
            "NaNs in X_train_normalized: 92471\n",
            "NaNs in X_test_normalized: 29987\n",
            "NaNs in X_train_normalized after replacement: 0\n",
            "NaNs in X_test_normalized after replacement: 0\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Kepler data stared at one field of space for four years,\n",
        "# Looking deeper at the K2 data can provide a new angle of analysing exoplanet features\n",
        "\n",
        "# RUN LOCALLY USING YOUR OWN DATA -\n",
        "# df = pd.read_csv(\n",
        "#     '/content/drive/MyDrive/KeplerExoplanet/k2_exoplanets_SUBSAMPLED_NASA_DATA.csv',\n",
        "#     comment='#'\n",
        "# )\n",
        "\n",
        "# OR TEST IT NOW WITH REAL NASA K2 SUBSAMPLED DATASET -\n",
        "# https://github.com/hazieon/Exoplanet-Detection/blob/dev/k2_exoplanets_SUBSAMPLED_NASA_DATA.csv\n",
        "df = pd.read_csv('https://drive.google.com/uc?id=1Afu7nOPzfdvhsbLldb98Il3cwZOWQ1Pi&export=download',\n",
        "  comment='#'\n",
        ")\n",
        "\n",
        "\n",
        "# PREPROCESSING DATA\n",
        "\n",
        "# Map disposition to three categories: 0=Confirmed, 1=Candidate, 2=False Positive / Refuted\n",
        "df['ExoplanetClass'] = df['disposition'].map({\n",
        "    'CONFIRMED': 0,\n",
        "    'CANDIDATE': 1,\n",
        "    'FALSE POSITIVE': 2,\n",
        "    'REFUTED': 2  # just in case some are labeled REFUTED\n",
        "})\n",
        "\n",
        "# Drop any rows that didn’t map (NaN in ExoplanetClass)\n",
        "df = df[df['ExoplanetClass'].notna()]\n",
        "\n",
        "# Ensure integer type\n",
        "df['ExoplanetClass'] = df['ExoplanetClass'].astype(int)\n",
        "\n",
        "# Define target\n",
        "y = df['ExoplanetClass']\n",
        "\n",
        "# Separate classes (just for info, optional)\n",
        "confirmed = df[df['ExoplanetClass'] == 0]\n",
        "candidate = df[df['ExoplanetClass'] == 1]\n",
        "false_positive = df[df['ExoplanetClass'] == 2]\n",
        "\n",
        "# Columns to drop (non-numeric or leaky)\n",
        "non_numeric_columns = [\n",
        "    'pl_name', 'hostname', 'disposition', 'disp_refname',\n",
        "    'discoverymethod', 'disc_facility', 'soltype', 'pl_refname',\n",
        "    'pl_bmassprov', 'st_refname', 'st_spectype', 'sy_refname',\n",
        "    'rastr', 'decstr', 'releasedate', 'rowupdate', 'pl_pubdate',\n",
        "    'ConfirmedExoplanet',   # <- remove leakage if present\n",
        "    'ExoplanetClass'        # <- target itself, don’t let it leak into X\n",
        "]\n",
        "\n",
        "# Keep only numeric features (drop columns safely if they exist)\n",
        "drop_cols_existing = [col for col in non_numeric_columns if col in df.columns]\n",
        "df_numeric = df.drop(columns=drop_cols_existing)\n",
        "\n",
        "# Target column separately (already defined as y)\n",
        "# Select numeric features\n",
        "numeric_cols = df_numeric.select_dtypes(include='number').columns\n",
        "X = df_numeric[numeric_cols]\n",
        "\n",
        "# Train/test split (stratified for better balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set distribution:\\n\", y_train.value_counts())\n",
        "print(\"Testing set distribution:\\n\", y_test.value_counts())\n",
        "\n",
        "# Normalisation\n",
        "scaler = StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# --- Save the fitted scaler ---\n",
        "# THE SCALER CAN BE FOUND HERE:\n",
        "# Download or open in your gdrive directory\n",
        "# https://github.com/hazieon/Exoplanet-Detection/blob/dev/kepler_exoplanet_scaler.pkl\n",
        "# https://drive.google.com/file/d/1l8uHenVOR5qW3PZUHFfoZLGWnk15lR2l/view?usp=drive_link\n",
        "scaler_path = \"/content/drive/MyDrive/KeplerExoplanet/kepler_exoplanet_scaler.pkl\"\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "print(\"✅ New scaler created and saved successfully!\")\n",
        "print(f\"Saved to: {scaler_path}\")\n",
        "print(f\"Scaler fitted on {len(scaler.feature_names_in_)} features.\")\n",
        "print(\"First 10 features:\", scaler.feature_names_in_[:10])\n",
        "\n",
        "\n",
        "# Check for NaNs and replace with 0 values\n",
        "print(\"NaNs in X_train_normalized:\", np.isnan(X_train_normalized).sum())\n",
        "print(\"NaNs in X_test_normalized:\", np.isnan(X_test_normalized).sum())\n",
        "X_train_normalized = np.nan_to_num(X_train_normalized, nan=0.0)\n",
        "X_test_normalized = np.nan_to_num(X_test_normalized, nan=0.0)\n",
        "print(\"NaNs in X_train_normalized after replacement:\", np.isnan(X_train_normalized).sum())\n",
        "print(\"NaNs in X_test_normalized after replacement:\", np.isnan(X_test_normalized).sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "w62DIm8lyBUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba734d3c-337d-4fc5-a62a-64e56b804aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set distribution:\n",
            " ExoplanetClass\n",
            "0    1688\n",
            "1    1009\n",
            "2     214\n",
            "Name: count, dtype: int64\n",
            "Testing set distribution:\n",
            " ExoplanetClass\n",
            "0    563\n",
            "1    336\n",
            "2     72\n",
            "Name: count, dtype: int64\n",
            "NaNs in X_train_normalized: 92471\n",
            "NaNs in X_test_normalized: 29987\n",
            "NaNs in X_train_normalized after replacement: 0\n",
            "NaNs in X_test_normalized after replacement: 0\n"
          ]
        }
      ],
      "source": [
        "# PREPROCESSING DATA\n",
        "\n",
        "# Map disposition to three categories: 0=Confirmed, 1=Candidate, 2=False Positive / Refuted\n",
        "df['ExoplanetClass'] = df['disposition'].map({\n",
        "    'CONFIRMED': 0,\n",
        "    'CANDIDATE': 1,\n",
        "    'FALSE POSITIVE': 2,\n",
        "    'REFUTED': 2  # just in case some are labeled REFUTED\n",
        "})\n",
        "\n",
        "# Drop any rows that didn’t map (NaN in ExoplanetClass)\n",
        "df = df[df['ExoplanetClass'].notna()]\n",
        "\n",
        "# Ensure integer type\n",
        "df['ExoplanetClass'] = df['ExoplanetClass'].astype(int)\n",
        "\n",
        "# Define target\n",
        "y = df['ExoplanetClass']\n",
        "\n",
        "\n",
        "# Separate classes (just for info, optional)\n",
        "confirmed = df[df['ExoplanetClass'] == 0]\n",
        "candidate = df[df['ExoplanetClass'] == 1]\n",
        "false_positive = df[df['ExoplanetClass'] == 2]\n",
        "\n",
        "# Columns to drop (non-numeric)\n",
        "non_numeric_columns = [\n",
        "    'pl_name', 'hostname', 'disposition', 'disp_refname',\n",
        "    'discoverymethod', 'disc_facility', 'soltype', 'pl_refname',\n",
        "    'pl_bmassprov', 'st_refname', 'st_spectype', 'sy_refname',\n",
        "    'rastr', 'decstr', 'releasedate', 'rowupdate', 'pl_pubdate'\n",
        "]\n",
        "\n",
        "# Keep only numeric features (drop columns safely if they exist)\n",
        "drop_cols_existing = [col for col in non_numeric_columns if col in df.columns]\n",
        "df_numeric = df.drop(columns=drop_cols_existing)\n",
        "\n",
        "# Target column separately\n",
        "y = df_numeric['ExoplanetClass']\n",
        "\n",
        "# Select numeric features\n",
        "numeric_cols = df_numeric.select_dtypes(include='number').columns\n",
        "feature_cols = [col for col in numeric_cols if col not in ['ExoplanetClass']]\n",
        "X = df_numeric[feature_cols]\n",
        "\n",
        "# Train/test split (stratified for better balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Training set distribution:\\n\", y_train.value_counts())\n",
        "print(\"Testing set distribution:\\n\", y_test.value_counts())\n",
        "\n",
        "# Normalisation\n",
        "scaler = StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Check for NaNs and replace with 0 values\n",
        "print(\"NaNs in X_train_normalized:\", np.isnan(X_train_normalized).sum())\n",
        "print(\"NaNs in X_test_normalized:\", np.isnan(X_test_normalized).sum())\n",
        "X_train_normalized = np.nan_to_num(X_train_normalized, nan=0.0)\n",
        "X_test_normalized = np.nan_to_num(X_test_normalized, nan=0.0)\n",
        "print(\"NaNs in X_train_normalized after replacement:\", np.isnan(X_train_normalized).sum())\n",
        "print(\"NaNs in X_test_normalized after replacement:\", np.isnan(X_test_normalized).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSnyQaQxmRvH"
      },
      "outputs": [],
      "source": [
        "# Check normalized training data\n",
        "print(\"\\n=== Normalized Training Data ===\")\n",
        "print(\"Shape of dataset:\", X_train_normalized.shape)\n",
        "print(\"Number of rows:\", X_train_normalized.shape[0])\n",
        "print(\"Number of columns:\", X_train_normalized.shape[1])\n",
        "print(\"Total NaNs in dataset:\", np.isnan(X_train_normalized).sum())\n",
        "print(\"All numeric?\", np.issubdtype(X_train_normalized.dtype, np.number))\n",
        "\n",
        "# Check normalized test data\n",
        "print(\"\\n=== Normalized Test Data ===\")\n",
        "print(\"Shape of dataset:\", X_test_normalized.shape)\n",
        "print(\"Number of rows:\", X_test_normalized.shape[0])\n",
        "print(\"Number of columns:\", X_test_normalized.shape[1])\n",
        "print(\"Total NaNs in dataset:\", np.isnan(X_test_normalized).sum())\n",
        "print(\"All numeric?\", np.issubdtype(X_test_normalized.dtype, np.number))\n",
        "\n",
        "print(\"\\nFeature statistics (train set):\\n\", pd.DataFrame(X_train).describe().T[['mean','std','min','max']].head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzFKGcuJrDUE"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# PREPROCESSING: CLASS WEIGHTS\n",
        "# Adding class weights to balance the minority class (confirmed exoplanets)\n",
        "# Preferred this strategy over oversampling, which would generate false data\n",
        "# =========================\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute balanced weight classes\n",
        "classes = np.unique(y_train)  # unique labels in training set\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "class_weights_dict = dict(zip(classes, class_weights))\n",
        "print(\"Class weights:\", class_weights_dict)\n",
        "\n",
        "# Sweet spot amplification: moderate boost for minority class\n",
        "# Multiply the minority class weight by 1.3 instead of 2.0 to avoid trashing Candidate class\n",
        "max_class = max(class_weights_dict, key=class_weights_dict.get)\n",
        "sweet_class_weights_dict = {k: (v*1.3 if k == max_class else v) for k, v in class_weights_dict.items()}\n",
        "print(\"Sweet-spot amplified class weights for minority class:\", sweet_class_weights_dict)\n",
        "\n",
        "# VISUALISE CLASS WEIGHTS\n",
        "plt.figure(figsize=(6,4))\n",
        "classes_labels = ['Confirmed', 'Candidate', 'False Positive']  # keep your original labels\n",
        "weights = [sweet_class_weights_dict[cls] for cls in classes]\n",
        "\n",
        "plt.bar(classes_labels, weights, color=['green', 'blue', 'purple'])\n",
        "plt.title('Sweet-Spot Amplified Class Weights for Training')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Weight')\n",
        "plt.ylim(0, max(weights) * 1.1)\n",
        "for i, w in enumerate(weights):\n",
        "    plt.text(i, w + 0.05, f'{w:.2f}', ha='center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu-ZHuWJ8DV7"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# RANDOM FOREST CLASSIFIER - Classical ML analysis\n",
        "# ==========================================================\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ==========================================================\n",
        "# We already have X_train_normalized, X_test_normalized, y_train, y_test from preprocessing\n",
        "# ==========================================================\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    min_samples_split=10,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    class_weight=sweet_class_weights_dict  # same weights as NN\n",
        ")\n",
        "\n",
        "# ==========================================================\n",
        "# RUNNING THE MODEL\n",
        "# ==========================================================\n",
        "# Fit the model\n",
        "rf.fit(X_train_normalized, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf.predict(X_test_normalized)\n",
        "\n",
        "# ==========================================================\n",
        "# EVALUATION\n",
        "# ==========================================================\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(\n",
        "    y_test, y_pred,\n",
        "    target_names=['Confirmed', 'Candidate', 'False Positive']\n",
        "))\n",
        "\n",
        "# ==========================================================\n",
        "# VISUALISATION OF RESULTS\n",
        "# ==========================================================\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(\n",
        "    cm, annot=True, fmt='d', cmap='Blues',\n",
        "    xticklabels=['Confirmed','Candidate','False Positive'],\n",
        "    yticklabels=['Confirmed','Candidate','False Positive']\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Random Forest Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# ==========================================================\n",
        "# FEATURE IMPORTANCES\n",
        "# ==========================================================\n",
        "readable_names = {\n",
        "    'sy_pnum': 'Number of planets in system',\n",
        "    'st_masserr1': 'Host star mass (+ variation) (Solar masses)',\n",
        "    'st_masserr2': 'Host star mass (- variation) (Solar masses)',\n",
        "    'sy_disterr1': 'System distance (+ variation) (pc)',\n",
        "    'sy_disterr2': 'System distance (- variation) (pc)',\n",
        "    'pl_orbsmaxerr1': 'Semi-major axis (+ variation) (AU)',\n",
        "    'pl_orbsmaxerr2': 'Semi-major axis (- variation) (AU)',\n",
        "    'pl_radjerr2': 'Planet radius (- variation) (R_Jupiter)',\n",
        "    'pl_radjerr1': 'Planet radius (+ variation) (R_Jupiter)',\n",
        "    'pl_radeerr1': 'Planet radius (+ variation) (R_Earth)',\n",
        "    'pl_radeerr2': 'Planet radius (- variation) (R_Earth)',\n",
        "    'pl_rade': 'Planet radius (R_Earth)',\n",
        "    'sy_dist': 'System distance (pc)',\n",
        "    'st_tefferr2': 'Host star temperature (- variation) (K)',\n",
        "    'st_tefferr1': 'Host star temperature (+ variation) (K)',\n",
        "    'st_mass': 'Host star mass (Solar masses)',\n",
        "    'pl_orbsmaxlim': 'Semi-major axis limit flag',\n",
        "    'pl_radj': 'Planetary radius (in Jupiter radii)',\n",
        "    'sy_kmag': 'Star brightness (infrared, K-band)',\n",
        "    'sy_gaiamagerr1': 'Gaia magnitude (+ variation)',\n",
        "    'sy_gaiamagerr2': 'Gaia magnitude (- variation)'\n",
        "}\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "readable_feature_names = [readable_names.get(f, f) for f in feature_names]\n",
        "\n",
        "# Build DataFrame of importances\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    'Feature': readable_feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Features by Importance:\")\n",
        "print(feat_imp_df.head(10))\n",
        "\n",
        "# Remove redundant features if any\n",
        "feat_imp_df = feat_imp_df[~feat_imp_df['Feature'].isin(['default_flag', 'disc_year'])]\n",
        "\n",
        "# Identify top feature\n",
        "top_feature = feat_imp_df.iloc[0]['Feature']\n",
        "top_feat_df = feat_imp_df[feat_imp_df['Feature'] == top_feature]\n",
        "other_feats_df = feat_imp_df[feat_imp_df['Feature'] != top_feature]\n",
        "\n",
        "# ==========================================================\n",
        "# VISUALISATION OF FEATURE IMPORTANCE\n",
        "# ==========================================================\n",
        "feat_imp_top5 = feat_imp_df.head(5)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Importance', y='Feature', data=feat_imp_top5)\n",
        "plt.title('Top Feature Importances')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x='Importance', y='Feature', data=other_feats_df.head(10))\n",
        "plt.title('Other Top Features (excluding the top feature)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u_gwXScuF78"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras import Input, regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train_normalized.shape[1],)),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "])\n",
        "\n",
        "# Add hidden layers\n",
        "model.add(Dense(units=36, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dense(units=18, activation='relu'))  # intermediate dense layer\n",
        "model.add(Dropout(0.4))  #  dropout for minority class learning & prevent overfitting\n",
        "model.add(Dense(units=18, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Add the output layer\n",
        "model.add(Dense(units=3, activation='softmax'))  # For three-way classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print the summary of the model\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxbAVosbvSbM"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_normalized,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    class_weight=sweet_class_weights_dict\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_prob = model.predict(X_test_normalized)\n",
        "y_pred = y_pred_prob.argmax(axis=1)  # Take the class with highest probability\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test_normalized, y_test)\n",
        "\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# Plot the model architecture and display it\n",
        "plot_file = 'model_plot.png'\n",
        "plot_model(model, to_file=plot_file, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# Display the plot inline\n",
        "display(Image(filename=plot_file))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9MqgjbFmFLT"
      },
      "outputs": [],
      "source": [
        "# VISUALISE THE RESULTS\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Classification report\n",
        "# Use zero_division=0 to avoid warnings when a class has no predicted samples\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    target_names=['Confirmed', 'Candidate', 'False Positive'],\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred, normalize='true')  # normalized per row\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(\n",
        "    cm, annot=True, fmt='.2f', cmap='Blues',\n",
        "    xticklabels=['Confirmed','Candidate','False Positive'],\n",
        "    yticklabels=['Confirmed','Candidate','False Positive']\n",
        ")\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Optionally, plot training curves\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.65, 1.0)\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, max(history.history['loss'] + history.history['val_loss']))\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "NZtilNJGlfvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133e8fe5-3049-4e5d-ec6d-7125c1bd2e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicting exoplanets from: /content/drive/MyDrive/KeplerExoplanet/K2_subsample_10_Exoplanets_unseen.csv\n",
            "\n",
            "Prediction Summary:\n",
            "         Prediction  Count\n",
            "0  Confirmed Planet     16\n",
            "1  Candidate Planet      5\n",
            "\n",
            "Full Predictions:\n",
            " Predicted_Label  Confidence       Prediction\n",
            "               0    0.999443 Confirmed Planet\n",
            "               0    0.998832 Confirmed Planet\n",
            "               0    0.997846 Confirmed Planet\n",
            "               0    0.997085 Confirmed Planet\n",
            "               1    0.711614 Candidate Planet\n",
            "               1    0.700792 Candidate Planet\n",
            "               1    0.711002 Candidate Planet\n",
            "               1    0.700792 Candidate Planet\n",
            "               1    0.884396 Candidate Planet\n",
            "               0    0.992896 Confirmed Planet\n",
            "               0    0.999796 Confirmed Planet\n",
            "               0    0.997780 Confirmed Planet\n",
            "               0    0.910095 Confirmed Planet\n",
            "               0    0.872632 Confirmed Planet\n",
            "               0    0.918003 Confirmed Planet\n",
            "               0    0.980238 Confirmed Planet\n",
            "               0    0.998146 Confirmed Planet\n",
            "               0    0.982095 Confirmed Planet\n",
            "               0    0.998997 Confirmed Planet\n",
            "               0    0.893215 Confirmed Planet\n",
            "               0    0.999579 Confirmed Planet\n",
            "\n",
            "Predicting exoplanets from: https://docs.google.com/spreadsheets/d/1EoxQOttHsFX77pHztU8fhanfZXN1wnn3hD1wVTDBSi0/export?format=csv\n",
            "\n",
            "Prediction Summary:\n",
            "         Prediction  Count\n",
            "0  Confirmed Planet     16\n",
            "1  Candidate Planet      5\n",
            "\n",
            "Full Predictions:\n",
            " Predicted_Label  Confidence       Prediction\n",
            "               0    0.999443 Confirmed Planet\n",
            "               0    0.998832 Confirmed Planet\n",
            "               0    0.997846 Confirmed Planet\n",
            "               0    0.997085 Confirmed Planet\n",
            "               1    0.711614 Candidate Planet\n",
            "               1    0.700792 Candidate Planet\n",
            "               1    0.711002 Candidate Planet\n",
            "               1    0.700792 Candidate Planet\n",
            "               1    0.884396 Candidate Planet\n",
            "               0    0.992896 Confirmed Planet\n",
            "               0    0.999796 Confirmed Planet\n",
            "               0    0.997780 Confirmed Planet\n",
            "               0    0.910095 Confirmed Planet\n",
            "               0    0.872632 Confirmed Planet\n",
            "               0    0.918003 Confirmed Planet\n",
            "               0    0.980238 Confirmed Planet\n",
            "               0    0.998146 Confirmed Planet\n",
            "               0    0.982095 Confirmed Planet\n",
            "               0    0.998997 Confirmed Planet\n",
            "               0    0.893215 Confirmed Planet\n",
            "               0    0.999579 Confirmed Planet\n"
          ]
        }
      ],
      "source": [
        "def predict_exoplanets(csv_path):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import joblib\n",
        "\n",
        "    print(f\"\\nPredicting exoplanets from: {csv_path}\")\n",
        "\n",
        "    global model  # model must already be loaded\n",
        "\n",
        "    # Load scaler\n",
        "    try:\n",
        "        scaler = joblib.load(\"/content/drive/MyDrive/KeplerExoplanet/kepler_exoplanet_scaler.pkl\")\n",
        "        # https://drive.google.com/file/d/1l8uHenVOR5qW3PZUHFfoZLGWnk15lR2l/view?usp=sharing\n",
        "        # https://github.com/hazieon/Exoplanet-Detection/blob/dev/kepler_exoplanet_scaler.pkl\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading scaler: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Load CSV\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, comment=\"#\")\n",
        "        df.columns = df.columns.str.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Drop non-numeric columns\n",
        "    drop_cols = [\n",
        "        'pl_name', 'hostname', 'disposition', 'disp_refname', 'discoverymethod',\n",
        "        'disc_facility', 'soltype', 'pl_refname', 'pl_bmassprov', 'st_refname',\n",
        "        'st_spectype', 'sy_refname', 'rastr', 'decstr', 'releasedate',\n",
        "        'rowupdate', 'pl_pubdate', 'ExoplanetClass'\n",
        "    ]\n",
        "    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\")\n",
        "\n",
        "    # Prepare features\n",
        "    expected = list(scaler.feature_names_in_)\n",
        "    df_features = pd.DataFrame({c: pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
        "                                if c in df.columns else np.zeros(len(df))\n",
        "                                for c in expected})\n",
        "    missing = [c for c in expected if c not in df.columns]\n",
        "    if missing:\n",
        "        print(f\"Warning: {len(missing)} missing features replaced with 0\")\n",
        "\n",
        "    # Predict\n",
        "    try:\n",
        "        X_scaled = scaler.transform(df_features)\n",
        "        preds = model.predict(X_scaled, verbose=0)\n",
        "\n",
        "        if preds.ndim > 1 and preds.shape[1] > 1:\n",
        "            labels = np.argmax(preds, axis=1)\n",
        "            conf = preds.max(axis=1)\n",
        "        else:\n",
        "            labels = (preds > 0.5).astype(int).flatten()\n",
        "            conf = preds.flatten()\n",
        "\n",
        "        label_map = {\n",
        "            0: \"Confirmed Planet\",\n",
        "            1: \"Candidate Planet\",\n",
        "            2: \"False Positive / Non-Planet\"\n",
        "        }\n",
        "\n",
        "        df[\"Predicted_Label\"] = labels\n",
        "        df[\"Confidence\"] = conf.astype(float)\n",
        "        df[\"Prediction\"] = df[\"Predicted_Label\"].map(label_map)\n",
        "\n",
        "        # Output\n",
        "        print(\"\\nPrediction Summary:\")\n",
        "        print(df[\"Prediction\"].value_counts().rename_axis(\"Prediction\").reset_index(name=\"Count\"))\n",
        "        print(\"\\nFull Predictions:\")\n",
        "        print(df[[\"Predicted_Label\", \"Confidence\", \"Prediction\"]].to_string(index=False))\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction failed: {e}\")\n",
        "        df[\"Predicted_Label\"] = \"Error\"\n",
        "        return df\n",
        "\n",
        "\n",
        "# Run on unseen data [locally using your own NASA data]\n",
        "results_df = predict_exoplanets(\n",
        "    \"/content/drive/MyDrive/KeplerExoplanet/K2_subsample_10_Exoplanets_unseen.csv\"\n",
        ")\n",
        "\n",
        "# # OR TEST IT NOW WITH REAL NASA K2 SUBSAMPLED DATASET -\n",
        "# https://github.com/hazieon/Exoplanet-Detection/blob/dev/K2_subsample_10_Exoplanets_unseen.csv\n",
        "results_df = predict_exoplanets('https://docs.google.com/spreadsheets/d/1EoxQOttHsFX77pHztU8fhanfZXN1wnn3hD1wVTDBSi0/export?format=csv'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "_EDe2qzSoLd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fd9271-0bad-40b0-d8b2-b7a5d85df1e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter CSV path or URL for new exoplanet data: https://docs.google.com/spreadsheets/d/1qAlEqBWW0rzLVbgqfz7ke8SoiLWateMPdVhGOqwGJQM/export?format=csv\n",
            "\n",
            "Predicting exoplanets from: https://docs.google.com/spreadsheets/d/1qAlEqBWW0rzLVbgqfz7ke8SoiLWateMPdVhGOqwGJQM/export?format=csv\n",
            "\n",
            "Prediction Summary:\n",
            "                    Prediction  Count\n",
            "0             Confirmed Planet     50\n",
            "1  False Positive / Non-Planet     31\n",
            "2             Candidate Planet     20\n",
            "\n",
            "Full Predictions:\n",
            " Predicted_Label  Confidence                  Prediction\n",
            "               1    0.713811            Candidate Planet\n",
            "               1    0.780447            Candidate Planet\n",
            "               1    0.780908            Candidate Planet\n",
            "               2    0.602072 False Positive / Non-Planet\n",
            "               1    0.922202            Candidate Planet\n",
            "               1    0.896644            Candidate Planet\n",
            "               1    0.765156            Candidate Planet\n",
            "               1    0.724309            Candidate Planet\n",
            "               1    0.723790            Candidate Planet\n",
            "               2    0.497605 False Positive / Non-Planet\n",
            "               2    0.622612 False Positive / Non-Planet\n",
            "               1    0.921404            Candidate Planet\n",
            "               1    0.588191            Candidate Planet\n",
            "               1    0.901799            Candidate Planet\n",
            "               0    0.999980            Confirmed Planet\n",
            "               0    0.999979            Confirmed Planet\n",
            "               0    0.999985            Confirmed Planet\n",
            "               1    0.904391            Candidate Planet\n",
            "               1    0.638213            Candidate Planet\n",
            "               1    0.713916            Candidate Planet\n",
            "               1    0.908930            Candidate Planet\n",
            "               1    0.974127            Candidate Planet\n",
            "               2    0.617010 False Positive / Non-Planet\n",
            "               2    0.733717 False Positive / Non-Planet\n",
            "               2    0.999825 False Positive / Non-Planet\n",
            "               2    0.950964 False Positive / Non-Planet\n",
            "               2    0.948203 False Positive / Non-Planet\n",
            "               2    0.638584 False Positive / Non-Planet\n",
            "               2    0.920295 False Positive / Non-Planet\n",
            "               2    0.808836 False Positive / Non-Planet\n",
            "               1    0.832667            Candidate Planet\n",
            "               2    0.999982 False Positive / Non-Planet\n",
            "               2    0.894004 False Positive / Non-Planet\n",
            "               2    0.999940 False Positive / Non-Planet\n",
            "               2    0.999677 False Positive / Non-Planet\n",
            "               2    0.558660 False Positive / Non-Planet\n",
            "               2    0.588836 False Positive / Non-Planet\n",
            "               2    0.743202 False Positive / Non-Planet\n",
            "               2    0.998744 False Positive / Non-Planet\n",
            "               2    0.944726 False Positive / Non-Planet\n",
            "               2    0.999712 False Positive / Non-Planet\n",
            "               2    0.998443 False Positive / Non-Planet\n",
            "               2    0.735580 False Positive / Non-Planet\n",
            "               1    0.715060            Candidate Planet\n",
            "               2    0.921580 False Positive / Non-Planet\n",
            "               2    0.999141 False Positive / Non-Planet\n",
            "               2    0.776720 False Positive / Non-Planet\n",
            "               2    0.973460 False Positive / Non-Planet\n",
            "               2    0.483981 False Positive / Non-Planet\n",
            "               2    0.764099 False Positive / Non-Planet\n",
            "               2    0.843782 False Positive / Non-Planet\n",
            "               2    0.576329 False Positive / Non-Planet\n",
            "               1    0.842072            Candidate Planet\n",
            "               1    0.937265            Candidate Planet\n",
            "               0    0.505274            Confirmed Planet\n",
            "               0    0.490067            Confirmed Planet\n",
            "               0    0.999978            Confirmed Planet\n",
            "               0    0.999852            Confirmed Planet\n",
            "               0    0.999942            Confirmed Planet\n",
            "               0    0.927495            Confirmed Planet\n",
            "               0    0.995954            Confirmed Planet\n",
            "               0    0.999997            Confirmed Planet\n",
            "               0    0.999994            Confirmed Planet\n",
            "               0    0.999995            Confirmed Planet\n",
            "               0    0.999999            Confirmed Planet\n",
            "               0    0.999993            Confirmed Planet\n",
            "               0    0.999999            Confirmed Planet\n",
            "               0    1.000000            Confirmed Planet\n",
            "               0    0.999981            Confirmed Planet\n",
            "               0    0.999979            Confirmed Planet\n",
            "               0    0.999993            Confirmed Planet\n",
            "               0    0.999995            Confirmed Planet\n",
            "               0    0.999997            Confirmed Planet\n",
            "               0    0.999957            Confirmed Planet\n",
            "               0    0.999991            Confirmed Planet\n",
            "               0    0.999993            Confirmed Planet\n",
            "               0    0.999991            Confirmed Planet\n",
            "               0    0.999954            Confirmed Planet\n",
            "               0    0.999981            Confirmed Planet\n",
            "               0    0.999993            Confirmed Planet\n",
            "               0    0.999994            Confirmed Planet\n",
            "               0    0.999904            Confirmed Planet\n",
            "               0    0.999985            Confirmed Planet\n",
            "               0    1.000000            Confirmed Planet\n",
            "               0    0.999988            Confirmed Planet\n",
            "               0    0.999998            Confirmed Planet\n",
            "               0    0.999996            Confirmed Planet\n",
            "               0    0.999641            Confirmed Planet\n",
            "               0    0.976940            Confirmed Planet\n",
            "               0    0.983513            Confirmed Planet\n",
            "               0    0.938725            Confirmed Planet\n",
            "               0    0.965780            Confirmed Planet\n",
            "               0    0.987371            Confirmed Planet\n",
            "               0    0.976940            Confirmed Planet\n",
            "               0    0.999808            Confirmed Planet\n",
            "               0    0.999641            Confirmed Planet\n",
            "               0    1.000000            Confirmed Planet\n",
            "               0    0.998580            Confirmed Planet\n",
            "               0    0.999424            Confirmed Planet\n",
            "               0    0.999424            Confirmed Planet\n",
            "               0    0.995272            Confirmed Planet\n",
            "\n",
            "==============================\n",
            "Full Predictions for All Rows\n",
            "==============================\n",
            "     Predicted_Label  Confidence                   Prediction\n",
            "0                  1    0.713811             Candidate Planet\n",
            "1                  1    0.780447             Candidate Planet\n",
            "2                  1    0.780908             Candidate Planet\n",
            "3                  2    0.602072  False Positive / Non-Planet\n",
            "4                  1    0.922202             Candidate Planet\n",
            "..               ...         ...                          ...\n",
            "96                 0    1.000000             Confirmed Planet\n",
            "97                 0    0.998580             Confirmed Planet\n",
            "98                 0    0.999424             Confirmed Planet\n",
            "99                 0    0.999424             Confirmed Planet\n",
            "100                0    0.995272             Confirmed Planet\n",
            "\n",
            "[101 rows x 3 columns]\n",
            "\n",
            "==============================\n",
            "Prediction Summary\n",
            "==============================\n",
            "                    Prediction  Number_of_Rows  Average_Confidence\n",
            "0             Candidate Planet              20            0.809265\n",
            "1             Confirmed Planet              50            0.974784\n",
            "2  False Positive / Non-Planet              31            0.810978\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------\n",
        "# Run on user-specified data (local file or URL)\n",
        "# ------------------------------\n",
        "user_csv_path = input(\"Enter CSV path or URL for new exoplanet data: \").strip()\n",
        "\n",
        "# Ensure it's a string\n",
        "if not isinstance(user_csv_path, str) or len(user_csv_path) == 0:\n",
        "    print(\"Invalid input. Please provide a CSV path or URL as a string.\")\n",
        "else:\n",
        "    results_df = predict_exoplanets(user_csv_path)\n",
        "\n",
        "    if results_df is not None and \"Prediction\" in results_df.columns:\n",
        "        print(\"\\n==============================\")\n",
        "        print(\"Full Predictions for All Rows\")\n",
        "        print(\"==============================\")\n",
        "        print(results_df[[\"Predicted_Label\", \"Confidence\", \"Prediction\"]].reset_index(drop=True))\n",
        "\n",
        "        # Count by category and average confidence\n",
        "        summary = results_df.groupby(\"Prediction\")[\"Confidence\"].agg([\"count\", \"mean\"]).reset_index()\n",
        "        summary.rename(columns={\"count\": \"Number_of_Rows\", \"mean\": \"Average_Confidence\"}, inplace=True)\n",
        "\n",
        "        print(\"\\n==============================\")\n",
        "        print(\"Prediction Summary\")\n",
        "        print(\"==============================\")\n",
        "        print(summary)\n",
        "\n",
        "# HELLO NASA FRIENDS :)\n",
        "# TEST OUT WITH THE CSV FILE BELOW OF UNSEEN EXOPLANET DATA -\n",
        "# use the CSV file path provided\n",
        "# 'https://docs.google.com/spreadsheets/d/1qAlEqBWW0rzLVbgqfz7ke8SoiLWateMPdVhGOqwGJQM/edit?usp=sharing'\n",
        "# https://github.com/hazieon/Exoplanet-Detection/blob/dev/K2_subsample_100_Exoplanets_unseen.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}